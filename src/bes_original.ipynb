{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd231dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "from utils import get_device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38e8a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = whisper.load_model(\"tiny\")\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "engligh_token = \"<|en|>\"\n",
    "welsh_token = 50297 #\"<|cy|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19a1f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the audio file\n",
    "FILE_PATH = \"../audio/MoreWelsh.m4a\"\n",
    "audio = whisper.load_audio(FILE_PATH)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "log_mel = whisper.log_mel_spectrogram(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "448d0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper prediction without fine tuning:  Sundin.wells. Anglesy.\n"
     ]
    }
   ],
   "source": [
    "options = whisper.DecodingOptions()\n",
    "response = whisper.decode(model, log_mel, options)\n",
    "print(\"whisper prediction without fine tuning: \", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c08aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before training ---\n",
      "Ids Target:  [50297, 50359, 50363, 441, 1661, 81, 471, 378, 36363, 11, 4521, 904, 2030, 50257]\n",
      "Ids Pred:  [50259, 50358, 50363, 318, 86, 81, 471, 378, 16495, 4521, 4521, 401, 72, 13, 50257]\n",
      "Text target:  <|cy|><|transcribe|><|notimestamps|> Llandrindod Wells, Anglesey<|endoftext|>\n",
      "Text pred:  <|en|><|translate|><|notimestamps|> Swrindod Wales Ang Angoli.<|endoftext|>\n",
      "Loss:  11.355557441711426\n"
     ]
    }
   ],
   "source": [
    "# Preparing input for target for the model to train on and learn\n",
    "ids = []\n",
    "ids += [tokenizer.sot]\n",
    "ids += [welsh_token]\n",
    "ids += [tokenizer.transcribe]\n",
    "ids += [tokenizer.no_timestamps]\n",
    "ids += tokenizer.encode(\" Llandrindod Wells, Anglesey\")\n",
    "ids += [tokenizer.eot]\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_tokens = torch.tensor(ids).unsqueeze(0)\n",
    "mel_unsqueezed = log_mel.unsqueeze(0) #.to(device)\n",
    "prediction = model(tokens=train_tokens, mel=mel_unsqueezed)\n",
    "target = train_tokens[:, 1:].contiguous()  # Skip the start token\n",
    "\n",
    "print(\"--- Before training ---\")\n",
    "print(\"Ids Target: \", target.squeeze().tolist())\n",
    "print(\"Ids Pred: \", torch.argmax(prediction, dim=-1).squeeze().tolist())\n",
    "print(\"Text target: \", tokenizer.decode(target.squeeze().tolist()))\n",
    "print(\"Text pred: \", tokenizer.decode(torch.argmax(prediction, dim=-1).squeeze().tolist()))\n",
    "loss = criterion(prediction.transpose(1, 2), train_tokens)\n",
    "print(\"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30560b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac4dff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Training count 11 ----\n",
      "--- After training ---\n",
      "Ids Target:  [50297, 50359, 50363, 441, 1661, 81, 471, 378, 36363, 11, 4521, 904, 2030, 50257]\n",
      "Ids Pred:  [50297, 50358, 50363, 441, 1661, 81, 471, 378, 36363, 11, 4521, 904, 2030, 50257]\n",
      "Text target:  <|cy|><|transcribe|><|notimestamps|> Llandrindod Wells, Anglesey<|endoftext|>\n",
      "Text pred:  <|cy|><|translate|><|notimestamps|> Llandrindod Wells, Anglesey<|endoftext|>\n",
      "Loss:  0.5504031777381897\n"
     ]
    }
   ],
   "source": [
    "# Training the model - re-run this cell to have it train multiple times\n",
    "training_count += 1\n",
    "print(f\"---- Training count {training_count} ----\")\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"--- After training ---\")\n",
    "model.eval()\n",
    "prediction = model(tokens=train_tokens, mel=mel_unsqueezed)\n",
    "prediction = prediction[:, :-1, :].contiguous()  # Remove the last token\n",
    "\n",
    "print(\"Ids Target: \", target.squeeze().tolist())\n",
    "print(\"Ids Pred: \", torch.argmax(prediction, dim=-1).squeeze().tolist())\n",
    "print(\"Text target: \", tokenizer.decode(target.squeeze().tolist()))\n",
    "print(\"Text pred: \", tokenizer.decode(torch.argmax(prediction, dim=-1).squeeze().tolist()))\n",
    "\n",
    "loss = criterion(prediction.transpose(1, 2), target)\n",
    "print(\"Loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6d80c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 50200:  alcanz\n",
      "Token 50201: éma\n",
      "Token 50202:  incense\n",
      "Token 50203:  harden\n",
      "Token 50204:  granting\n",
      "Token 50205:  Nai\n",
      "Token 50206:  Firma\n",
      "Token 50207:  hypoc\n",
      "Token 50208: job\n",
      "Token 50209:  RH\n",
      "Token 50210: zur\n",
      "Token 50211: иля\n",
      "Token 50212:  ź\n",
      "Token 50213:  dares\n",
      "Token 50214: anh\n",
      "Token 50215:  만큼\n",
      "Token 50216:  cuestión\n",
      "Token 50217:  Lima\n",
      "Token 50218: 景\n",
      "Token 50219:  assunto\n",
      "Token 50220:  IPO\n",
      "Token 50221:  Bengal\n",
      "Token 50222:  Bier\n",
      "Token 50223:  psyche\n",
      "Token 50224:  acquainted\n",
      "Token 50225:  Gün\n",
      "Token 50226: ози\n",
      "Token 50227: ścią\n",
      "Token 50228: AG\n",
      "Token 50229:  malfunction\n",
      "Token 50230:  asteroids\n",
      "Token 50231: irez\n",
      "Token 50232: amorph\n",
      "Token 50233:  сотруд\n",
      "Token 50234:  freshwater\n",
      "Token 50235:  arran\n",
      "Token 50236:  пры\n",
      "Token 50237: ног\n",
      "Token 50238:  diabetic\n",
      "Token 50239:  قال\n",
      "Token 50240:  oppress\n",
      "Token 50241:  capacitance\n",
      "Token 50242: performance\n",
      "Token 50243: crates\n",
      "Token 50244:  apostle\n",
      "Token 50245:  JEN\n",
      "Token 50246: OULD\n",
      "Token 50247: Intro\n",
      "Token 50248:  stalls\n",
      "Token 50249:  ABOUT\n",
      "Token 50250: cticamente\n",
      "Token 50251:  diligent\n",
      "Token 50252:  manifests\n",
      "Token 50253:  Pakistani\n",
      "Token 50254:  ('\n",
      "Token 50255: 场\n",
      "Token 50256: \n",
      "Token 50257: <|endoftext|>\n",
      "Token 50258: <|startoftranscript|>\n",
      "Token 50259: <|en|>\n",
      "Token 50260: <|zh|>\n",
      "Token 50261: <|de|>\n",
      "Token 50262: <|es|>\n",
      "Token 50263: <|ru|>\n",
      "Token 50264: <|ko|>\n",
      "Token 50265: <|fr|>\n",
      "Token 50266: <|ja|>\n",
      "Token 50267: <|pt|>\n",
      "Token 50268: <|tr|>\n",
      "Token 50269: <|pl|>\n",
      "Token 50270: <|ca|>\n",
      "Token 50271: <|nl|>\n",
      "Token 50272: <|ar|>\n",
      "Token 50273: <|sv|>\n",
      "Token 50274: <|it|>\n",
      "Token 50275: <|id|>\n",
      "Token 50276: <|hi|>\n",
      "Token 50277: <|fi|>\n",
      "Token 50278: <|vi|>\n",
      "Token 50279: <|he|>\n",
      "Token 50280: <|uk|>\n",
      "Token 50281: <|el|>\n",
      "Token 50282: <|ms|>\n",
      "Token 50283: <|cs|>\n",
      "Token 50284: <|ro|>\n",
      "Token 50285: <|da|>\n",
      "Token 50286: <|hu|>\n",
      "Token 50287: <|ta|>\n",
      "Token 50288: <|no|>\n",
      "Token 50289: <|th|>\n",
      "Token 50290: <|ur|>\n",
      "Token 50291: <|hr|>\n",
      "Token 50292: <|bg|>\n",
      "Token 50293: <|lt|>\n",
      "Token 50294: <|la|>\n",
      "Token 50295: <|mi|>\n",
      "Token 50296: <|ml|>\n",
      "Token 50297: <|cy|>\n",
      "Token 50298: <|sk|>\n",
      "Token 50299: <|te|>\n",
      "Token 50300: <|fa|>\n",
      "Token 50301: <|lv|>\n",
      "Token 50302: <|bn|>\n",
      "Token 50303: <|sr|>\n",
      "Token 50304: <|az|>\n",
      "Token 50305: <|sl|>\n",
      "Token 50306: <|kn|>\n",
      "Token 50307: <|et|>\n",
      "Token 50308: <|mk|>\n",
      "Token 50309: <|br|>\n",
      "Token 50310: <|eu|>\n",
      "Token 50311: <|is|>\n",
      "Token 50312: <|hy|>\n",
      "Token 50313: <|ne|>\n",
      "Token 50314: <|mn|>\n",
      "Token 50315: <|bs|>\n",
      "Token 50316: <|kk|>\n",
      "Token 50317: <|sq|>\n",
      "Token 50318: <|sw|>\n",
      "Token 50319: <|gl|>\n",
      "Token 50320: <|mr|>\n",
      "Token 50321: <|pa|>\n",
      "Token 50322: <|si|>\n",
      "Token 50323: <|km|>\n",
      "Token 50324: <|sn|>\n",
      "Token 50325: <|yo|>\n",
      "Token 50326: <|so|>\n",
      "Token 50327: <|af|>\n",
      "Token 50328: <|oc|>\n",
      "Token 50329: <|ka|>\n",
      "Token 50330: <|be|>\n",
      "Token 50331: <|tg|>\n",
      "Token 50332: <|sd|>\n",
      "Token 50333: <|gu|>\n",
      "Token 50334: <|am|>\n",
      "Token 50335: <|yi|>\n",
      "Token 50336: <|lo|>\n",
      "Token 50337: <|uz|>\n",
      "Token 50338: <|fo|>\n",
      "Token 50339: <|ht|>\n",
      "Token 50340: <|ps|>\n",
      "Token 50341: <|tk|>\n",
      "Token 50342: <|nn|>\n",
      "Token 50343: <|mt|>\n",
      "Token 50344: <|sa|>\n",
      "Token 50345: <|lb|>\n",
      "Token 50346: <|my|>\n",
      "Token 50347: <|bo|>\n",
      "Token 50348: <|tl|>\n",
      "Token 50349: <|mg|>\n",
      "Token 50350: <|as|>\n",
      "Token 50351: <|tt|>\n",
      "Token 50352: <|haw|>\n",
      "Token 50353: <|ln|>\n",
      "Token 50354: <|ha|>\n",
      "Token 50355: <|ba|>\n",
      "Token 50356: <|jw|>\n",
      "Token 50357: <|su|>\n",
      "Token 50358: <|translate|>\n",
      "Token 50359: <|transcribe|>\n",
      "Token 50360: <|startoflm|>\n",
      "Token 50361: <|startofprev|>\n",
      "Token 50362: <|nospeech|>\n",
      "Token 50363: <|notimestamps|>\n",
      "Token 50364: \n",
      "Token 50365: \n",
      "Token 50366: \n",
      "Token 50367: \n",
      "Token 50368: \n",
      "Token 50369: \n",
      "Token 50370: \n",
      "Token 50371: \n",
      "Token 50372: \n",
      "Token 50373: \n",
      "Token 50374: \n",
      "Token 50375: \n",
      "Token 50376: \n",
      "Token 50377: \n",
      "Token 50378: \n",
      "Token 50379: \n",
      "Token 50380: \n",
      "Token 50381: \n",
      "Token 50382: \n",
      "Token 50383: \n",
      "Token 50384: \n",
      "Token 50385: \n",
      "Token 50386: \n",
      "Token 50387: \n",
      "Token 50388: \n",
      "Token 50389: \n",
      "Token 50390: \n",
      "Token 50391: \n",
      "Token 50392: \n",
      "Token 50393: \n",
      "Token 50394: \n",
      "Token 50395: \n",
      "Token 50396: \n",
      "Token 50397: \n",
      "Token 50398: \n",
      "Token 50399: \n",
      "Token 50400: \n",
      "Token 50401: \n",
      "Token 50402: \n",
      "Token 50403: \n",
      "Token 50404: \n",
      "Token 50405: \n",
      "Token 50406: \n",
      "Token 50407: \n",
      "Token 50408: \n",
      "Token 50409: \n",
      "Token 50410: \n",
      "Token 50411: \n",
      "Token 50412: \n",
      "Token 50413: \n",
      "Token 50414: \n",
      "Token 50415: \n",
      "Token 50416: \n",
      "Token 50417: \n",
      "Token 50418: \n",
      "Token 50419: \n",
      "Token 50420: \n",
      "Token 50421: \n",
      "Token 50422: \n",
      "Token 50423: \n",
      "Token 50424: \n",
      "Token 50425: \n",
      "Token 50426: \n",
      "Token 50427: \n",
      "Token 50428: \n",
      "Token 50429: \n",
      "Token 50430: \n",
      "Token 50431: \n",
      "Token 50432: \n",
      "Token 50433: \n",
      "Token 50434: \n",
      "Token 50435: \n",
      "Token 50436: \n",
      "Token 50437: \n",
      "Token 50438: \n",
      "Token 50439: \n",
      "Token 50440: \n",
      "Token 50441: \n",
      "Token 50442: \n",
      "Token 50443: \n",
      "Token 50444: \n",
      "Token 50445: \n",
      "Token 50446: \n",
      "Token 50447: \n",
      "Token 50448: \n",
      "Token 50449: \n",
      "Token 50450: \n",
      "Token 50451: \n",
      "Token 50452: \n",
      "Token 50453: \n",
      "Token 50454: \n",
      "Token 50455: \n",
      "Token 50456: \n",
      "Token 50457: \n",
      "Token 50458: \n",
      "Token 50459: \n",
      "Token 50460: \n",
      "Token 50461: \n",
      "Token 50462: \n",
      "Token 50463: \n",
      "Token 50464: \n",
      "Token 50465: \n",
      "Token 50466: \n",
      "Token 50467: \n",
      "Token 50468: \n",
      "Token 50469: \n",
      "Token 50470: \n",
      "Token 50471: \n",
      "Token 50472: \n",
      "Token 50473: \n",
      "Token 50474: \n",
      "Token 50475: \n",
      "Token 50476: \n",
      "Token 50477: \n",
      "Token 50478: \n",
      "Token 50479: \n",
      "Token 50480: \n",
      "Token 50481: \n",
      "Token 50482: \n",
      "Token 50483: \n",
      "Token 50484: \n",
      "Token 50485: \n",
      "Token 50486: \n",
      "Token 50487: \n",
      "Token 50488: \n",
      "Token 50489: \n",
      "Token 50490: \n",
      "Token 50491: \n",
      "Token 50492: \n",
      "Token 50493: \n",
      "Token 50494: \n",
      "Token 50495: \n",
      "Token 50496: \n",
      "Token 50497: \n",
      "Token 50498: \n",
      "Token 50499: \n"
     ]
    }
   ],
   "source": [
    "### find welsh token -> 50297: <|cy|>\n",
    "token_english = 50259\n",
    "\n",
    "for i in range(50200, 50500):\n",
    "    token = tokenizer.decode([i])\n",
    "    print(f\"Token {i}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2c8c1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the audio file\n",
    "FILE_PATH = \"../audio/Clem--Bes.m4a\"\n",
    "audio = whisper.load_audio(FILE_PATH)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "log_mel = whisper.log_mel_spectrogram(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dce08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before training ---\n",
      "Ids Target:  [50259, 50359, 50363, 2425, 11, 452, 1315, 307, 8190, 13, 50257]\n",
      "Ids Pred:  [50259, 50359, 50363, 2425, 11, 452, 1315, 307, 14011, 13, 50257, 50257]\n",
      "Text target:  <|en|><|transcribe|><|notimestamps|> Hello, my name is Bes.<|endoftext|>\n",
      "Text pred:  <|en|><|transcribe|><|notimestamps|> Hello, my name is Beth.<|endoftext|><|endoftext|>\n",
      "Loss:  12.903071403503418\n"
     ]
    }
   ],
   "source": [
    "# Preparing input for target for the model to train on and learn\n",
    "ids = []\n",
    "ids += [tokenizer.sot]\n",
    "ids += [tokenizer.language_token]\n",
    "ids += [tokenizer.transcribe]\n",
    "ids += [tokenizer.no_timestamps]\n",
    "ids += tokenizer.encode(\" Hello, my name is Bes.\")\n",
    "ids += [tokenizer.eot]\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_tokens = torch.tensor(ids).unsqueeze(0)\n",
    "mel_unsqueezed = log_mel.unsqueeze(0) #.to(device)\n",
    "prediction = model(tokens=train_tokens, mel=mel_unsqueezed)\n",
    "target = train_tokens[:, 1:].contiguous()  # Skip the start token\n",
    "\n",
    "print(\"--- Finetuned to Welsh ---\")\n",
    "print(\"Ids Target: \", target.squeeze().tolist())\n",
    "print(\"Ids Pred: \", torch.argmax(prediction, dim=-1).squeeze().tolist())\n",
    "print(\"Text target: \", tokenizer.decode(target.squeeze().tolist()))\n",
    "print(\"Text pred: \", tokenizer.decode(torch.argmax(prediction, dim=-1).squeeze().tolist()))\n",
    "loss = criterion(prediction.transpose(1, 2), train_tokens)\n",
    "print(\"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77701c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [50259, 50359, 50363, 2425, 11, 452, 1315, 307, 8190, 13, 50257]\n",
    "pred = [50259, 50359, 50363, 2425, 11, 452, 1315, 307, 14011, 13, 50257, 50257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96631c5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pred_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pred[:\u001b[38;5;28mlen\u001b[39m(target)])  \u001b[38;5;66;03m# Match target length\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Simulate logits: one-hot encode pred_tensor for demonstration\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50363\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Reshape target for loss function\u001b[39;00m\n\u001b[1;32m     11\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m target_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Convert lists to tensors\n",
    "target_tensor = torch.tensor(target)\n",
    "pred_tensor = torch.tensor(pred[:len(target)])  # Match target length\n",
    "\n",
    "# Simulate logits: one-hot encode pred_tensor for demonstration\n",
    "logits = F.one_hot(pred_tensor, num_classes=50363).float().unsqueeze(0)\n",
    "\n",
    "# Reshape target for loss function\n",
    "target_tensor = target_tensor.unsqueeze(0)\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits, target_tensor)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc6899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
